{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51c8336",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> 1eme Exemple </h2>  \n",
    "<h6 align=\"right\" > réalisé par : Farah Ftouhi , Seif Barouni & Aziz Bel Hadj Yahia</h6>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4f7bb",
   "metadata": {},
   "source": [
    "Bienvenue dans ce notebook sur le module sklearn.preprocessing! Dans ce notebook, nous allons passer en revue différentes méthodes de prétraitement de données que vous pouvez utiliser pour nettoyer, normaliser et transformer vos données avant de les utiliser pour entraîner des modèles.\n",
    "\n",
    "Pour commencer, nous allons importer les bibliothèques nécessaires pour ce notebook. Nous avons besoin de `numpy`, `pandas` et `sklearn.preprocessing` pour effectuer les opérations de prétraitement de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78a7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08525a9a",
   "metadata": {},
   "source": [
    "Le code suivant est utilisé pour charger un ensemble de données à partir d'un fichier CSV. Le fichier CSV est lu à l'aide de la fonction `read_csv()` de la bibliothèque `pandas`. Le chemin du fichier est spécifié en tant qu'argument de la fonction. Le contenu du fichier est stocké dans un objet `DataFrame` appelé `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355d87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Charger le dataset\n",
    "df = pd.read_csv('netflix_titles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1983bc",
   "metadata": {},
   "source": [
    "Le code suivant traite les valeurs manquantes dans l'ensemble de données. Tout d'abord, il est important de noter que les valeurs manquantes peuvent affecter les résultats de nos analyses et de nos modèles. Il est donc important de les traiter avant d'aller plus loin.\n",
    "\n",
    "Nous avons deux options pour gérer les valeurs manquantes: soit nous pouvons supprimer les lignes qui contiennent des valeurs manquantes, soit nous pouvons remplacer les valeurs manquantes par des valeurs imputées.\n",
    "\n",
    "Dans ce cas, nous utilisons l'imputation pour remplacer les valeurs manquantes. Nous créons une instance de la classe `SimpleImputer` de la bibliothèque `sklearn.impute` en spécifiant la stratégie `most_frequent`. Cela signifie que les valeurs manquantes seront remplacées par la valeur la plus fréquente dans la colonne respective.\n",
    "\n",
    "Ensuite, nous utilisons la méthode `fit_transform()` de l'objet imputer pour remplacer les valeurs manquantes dans l'ensemble de données `df`. Le résultat est un tableau NumPy qui est ensuite converti en un objet DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c10ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Gérer les valeurs manquantes\n",
    "# Supprimer toutes les lignes contenant des valeurs manquantes\n",
    "#df.dropna(inplace=True)\n",
    "# Ou utiliser SimpleImputer avec la stratégie de most_frequent (la plus fréquente)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df = imputer.fit_transform(df)\n",
    "\n",
    "df = pd.DataFrame(df, columns=['show_id', 'type', 'title', 'director', 'cast', 'country', 'date_added', 'release_year', 'rating', 'duration', 'listed_in', 'description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5044489",
   "metadata": {},
   "source": [
    "Puis, nous vérifions s'il reste des valeurs manquantes dans l'ensemble de données en utilisant la méthode `isnull().sum()`. Cela nous donne le nombre total de valeurs manquantes pour chaque colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a0f5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_id         0\n",
      "type            0\n",
      "title           0\n",
      "director        0\n",
      "cast            0\n",
      "country         0\n",
      "date_added      0\n",
      "release_year    0\n",
      "rating          0\n",
      "duration        0\n",
      "listed_in       0\n",
      "description     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vérifier s'il y a des valeurs manquantes\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb512b6f",
   "metadata": {},
   "source": [
    "Le code ci-dessous est utilisé pour convertir les variables catégorielles de l'ensemble de données en variables numériques, ce qui est souvent nécessaire pour l'analyse et la modélisation. Nous utilisons une méthode appelée encodage de labels pour effectuer cette conversion.\n",
    "\n",
    "Tout d'abord, nous créons une instance de la classe `LabelEncoder` de la bibliothèque `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22bf56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d171d8",
   "metadata": {},
   "source": [
    "Nous convertissons ensuite la colonne `\"release_year\"` en un type numérique en utilisant la méthode `pd.to_numeric()` de la bibliothèque `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbfa01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['release_year'] = pd.to_numeric(df['release_year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2be729",
   "metadata": {},
   "source": [
    "Nous identifions ensuite les colonnes non numériques dans l'ensemble de données en utilisant la méthode `select_dtypes()` de la bibliothèque `pandas` et nous stockons ces colonnes dans une variable appelée `non_numeric_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da26a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_cols = df.select_dtypes(exclude=np.number).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9ff50",
   "metadata": {},
   "source": [
    "Nous utilisons ensuite une boucle for pour parcourir toutes les colonnes non numériques et appliquer l'encodage de labels. La méthode `fit_transform()` de l'objet `LabelEncoder` est utilisée pour effectuer l'encodage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64374deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in non_numeric_cols:\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657754c",
   "metadata": {},
   "source": [
    "Enfin, nous affichons les colonnes qui vont être encodées en utilisant la méthode `print()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9777da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      show_id  type  title  director  cast  country  date_added  release_year  \\\n",
      "0           0     0   1975      2295  1699      603        1711          2020   \n",
      "1        1111     1   1091      3392   409      426        1706          2021   \n",
      "2        2222     1   2651      2105  6296      603        1706          2021   \n",
      "3        3333     1   3506      3392  1699      603        1706          2021   \n",
      "4        4444     1   3861      3392  4815      251        1706          2021   \n",
      "...       ...   ...    ...       ...   ...      ...         ...           ...   \n",
      "8802     8671     0   8770       979  4677      603        1419          2007   \n",
      "8803     8672     1   8773      3392  1699      603         788          2018   \n",
      "8804     8673     0   8774      3631  3231      603        1366          2009   \n",
      "8805     8674     0   8777      3247  7061      603         665          2006   \n",
      "8806     8675     0   8781      2926  7297      251        1127          2015   \n",
      "\n",
      "      rating  duration  listed_in  description  \n",
      "0          7       210        274         2577  \n",
      "1         11       110        414         1762  \n",
      "2         11         0        242         7341  \n",
      "3         11         0        297         3617  \n",
      "4         11       110        393         4416  \n",
      "...      ...       ...        ...          ...  \n",
      "8802       8        70        269          895  \n",
      "8803      14       110        424         8483  \n",
      "8804       8       206        207         5228  \n",
      "8805       6       206        125         3315  \n",
      "8806       9        16        328         1004  \n",
      "\n",
      "[8807 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vérifier si les données sont encodées\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9beb5",
   "metadata": {},
   "source": [
    "Dans cet exemple, nous avons utilisé la fonction `train_test_split` de la bibliothèque `sklearn.model_selection` pour diviser l'ensemble de données en un ensemble de données d'entraînement et un ensemble de données de test.\n",
    "\n",
    "La première étape consiste à définir les variables indépendantes (X) et la variable dépendante (y) que nous voulons prédire. Nous utilisons la méthode `iloc()` de la bibliothèque `pandas` pour extraire les colonnes nécessaires de l'ensemble de données. Dans cet exemple, nous avons extrait les deux premières colonnes de l'ensemble de données pour X et la troisième colonne pour y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69508058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Diviser les données en ensembles d'entraînement et de test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, 0:2]\n",
    "y = df.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60e3bf",
   "metadata": {},
   "source": [
    "Nous avons ensuite utilisé la fonction `train_test_split()` pour diviser l'ensemble de données en ensembles d'entraînement et de test. Cette fonction prend en entrée les variables indépendantes (X) et la variable dépendante (y) ainsi que le pourcentage de données que nous voulons réserver pour l'ensemble de test (dans cet exemple, 20% de l'ensemble de données). Nous avons également utilisé le paramètre `random_state` pour garantir que les résultats sont reproductibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e9638c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5206c1",
   "metadata": {},
   "source": [
    "Enfin, nous avons assigné les ensembles d'entraînement et de test aux variables `X_train`, `X_test`, `y_train` et `y_test`. Ces variables seront utilisées pour entraîner et tester nos modèles.\n",
    "\n",
    "Il est important de diviser l'ensemble de données en ensembles d'entraînement et de test pour évaluer la performance de notre modèle. En utilisant l'ensemble d'entraînement pour entraîner notre modèle, nous pouvons évaluer sa performance sur l'ensemble de test, qui contient des données inconnues pour le modèle. Cela nous permet de mesurer la capacité du modèle à généraliser à de nouvelles données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169dd8e",
   "metadata": {},
   "source": [
    "Le bloc de code suivant met à l'échelle les données en utilisant `StandardScaler` du module `sklearn.preprocessing`. `StandardScaler` met les données à l'échelle pour avoir une moyenne de zéro et un écart type de un. C'est important car cela permet aux caractéristiques avec des échelles plus grandes d'être traitées de manière égale que les caractéristiques avec des échelles plus petites. Dans ce cas, `X_train` et `X_test` sont modifiés pour être mises à l'échelle. fit_transform est utilisé sur `X_train` pour ajuster la mise à l'échelle des données d'apprentissage, tandis que transform est utilisé sur `X_test` pour appliquer la même mise à l'échelle à ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b68f8daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22517855 -0.65786337]\n",
      " [ 0.1543522   1.52007247]\n",
      " [ 0.2786918  -0.65786337]\n",
      " ...\n",
      " [ 0.18976538  1.52007247]\n",
      " [ 1.596849   -0.65786337]\n",
      " [ 1.0117446  -0.65786337]]\n",
      "[[ 0.00601033 -0.65786337]\n",
      " [-0.69674453 -0.65786337]\n",
      " [ 0.23501555  1.52007247]\n",
      " ...\n",
      " [ 1.31905229 -0.65786337]\n",
      " [ 0.95744439 -0.65786337]\n",
      " [-0.7148446  -0.65786337]]\n"
     ]
    }
   ],
   "source": [
    "# 6. Mise à l'échelle des données\n",
    "# Mise à l'échelle des données en utilisant StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Création de l'objet StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Mise à l'échelle des données d'apprentissage\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Mise à l'échelle des données de test en utilisant la même transformation\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3ba04",
   "metadata": {},
   "source": [
    "Le bloc de code suivant utilise `SelectKBest` du module `sklearn.feature_selection` pour sélectionner les deux meilleures caractéristiques. La fonction `chi2` est utilisée comme méthode de score pour `SelectKBest`. `X_train` et `X_test` sont également corrigées pour ne pas avoir de valeurs négatives. Ensuite, `fit_transform` est utilisé pour adapter et transformer `X_train` avec la sélection des deux meilleures caractéristiques, tandis que `fit_transform` est également utilisé pour transformer `X_test` avec la même sélection de deux meilleures caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9350681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Sélection des caractéristiques\n",
    "# Sélection des deux meilleures caractéristiques à l'aide de SelectKBest et chi2\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Correction des valeurs négatives\n",
    "X_train = X_train - X_train.min()\n",
    "X_test = X_test - X_test.min()\n",
    "\n",
    "# Sélection des deux meilleures caractéristiques pour les données d'apprentissage et de test\n",
    "X_train = SelectKBest(chi2, k=2).fit_transform(X_train, y_train)\n",
    "X_test = SelectKBest(chi2, k=2).fit_transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b64f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
